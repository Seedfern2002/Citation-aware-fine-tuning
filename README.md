# Citation-Aware Fine-Tuning with ALCE

This repository implements a supervised fine-tuning (SFT) pipeline for training large language models to produce citation-grounded answers. With dataset from [Enabling Large Language Models to Generate Text with Citations](https://arxiv.org/abs/2305.14627).


## File Descriptions


### `utils/prepare_dataset.py`

Constructs a citation-aware SFT dataset by determining whether documents support each answer or claim using ELI5 and QAMPari dataset. 

**Usage**

**Notes**
- Because the data composed of 100 chunks would be too long (>10000 tokens) for fine-tuning, this code will only randomly sample 10 chunks to construct data.
```bash
python utils/prepare_dataset.py \
  --input_files data/raw/ALCE/data/eli5_eval_bm25_top100.json data/raw/ALCE/data/qampari_eval_gtr_top100.json \
  --output data/processed/dataset.jsonl \
  --max_samples 100 \
  --seed 42
```


### `utils/prepare_dataset_oracle.py`

Constructs an SFT dataset using **only oracle files** from ALCE.

**Notes**
- The generated dataset may be biased, since oracle files contain only the top-5 matching documents.

**Usage**
```bash
python utils/prepare_dataset_oracle.py \
  --input_files data/raw/ALCE/data/eli5_eval_bm25_top100_reranked_oracle.json data/raw/ALCE/data/qampari_eval_gtr_top100_reranked_oracle.json \
  --output data/processed/dataset.jsonl \
  --max_samples 100 \
  --seed 42
```

### `data/processed/`

Stores generated SFT datasets.

**Files**
- `dataset.jsonl`  
  Dataset generated by utils/prepare_dataset.py

- `dataset_oracle.jsonl`  
  Dataset generated by utils/prepare_dataset_oracle.py

### `scripts/fine_tune_qwen3.slurm`
SLURM submission script for distributed training of Qwen/Qwen3-1.7B.

**Usage**
```bash
uv -sync
```
```bash
sbatch scripts/fine_tune_qwen3.slurm
```

